# 2020/05/12

Initial meeting for CNV HTP on MOC



*   Introductions
*   Program Intro/Overview
    *   The purpose is that we get the right feedback in terms of features, bugs, etc as we go forward with our initial GA.
    *   Expectations: A Deployment Kit to stand up the product in a standard way as quickly and frictionless as possible so that we can execute the test plan with the use cases of interest we would like you to execute as part of this program.
        *   This does not mean that we can’t go out and try other things outside of the test plan. We expect the MOC team to stretch this test plan.
        *   Our plan would be to start with an install using the prescribed plan and then figure out ways to do it automatically.
    *   A process to report issues as part of the customer portal.
    *   Our timing for having the test kit available within the next one or two weeks.
    *   We’ll have Ian engaged with getting the environment setup (fortunately w/o any hiccups due to the COVID-19 situation).
    *   [Recommended HW and environment requirements for the CNV HTB](https://docs.google.com/document/d/1ntS4Iwg6aCulVz2s8110oxp7UdeYQY7odSklEex04_Y/edit).
        *   We have 6 Dell RX620 servers.
        *   Questions on HW support:
            *   **Networking**
                *   Cisco nodes for which RHEL doesn’t have the right drivers.
                *   Do we have the correct iPXE driver support on the Dell nodes (Lars to clarify…)
                    *   NIC models
                *   [ ] Lars: Provide more specific details about our HW configuration.
            *   **Storage**
                *   Initially
                    *   Use Local Storage - 600 GB HDDs
                *   Subsequently
                    *   Use External Storage using OCS 4.4 - Ceph Clusters
            *   **Memory**
                *   128 GBs
        *   Info about network configuration
            *   We have administrative access to network infrastructure (routers, switches, etc).
        *   Info about Workloads
            *   We are looking to move our management plane to a single platform.
            *   We would love to set up the infrastructure for the New England Research Cloud to run on OpenShift Container Platform + OpenShift virtualization.
        *   Info about installation:
            *   Initially, we’ll start with the current available OCP and CNV  version. This environment is expected to be burned down, however it would be better to do the upgrade.
            *   Subsequently, we’ll move up to OCP 4.5 / CNV 2.4.
    *   Next Steps
        *   Get started with the OCP IPI BM installation as soon as possible.
        *   Expect the Deployment Kit to be available within two weeks.

**[ ] Rick: **set another follow up meeting in two weeks.



# 2020-05-06 PM

## UPI vs IPI

UPI is more mature, but IPI is the preferred option, especially if you want to
add things after deploying the cluster. IPI looks like the way we’ll go.

## Requirements

Minimal required configuration:

- 1 Bootstrap VM
- 3 masters (to get the cluster to come up)
- 2 Workers (more can be added later)

The greater uniformity, the greater chance of success. Nodes within a role must be identical. NIC names (at least for provisioning NIC) must be identical across all roles.

There must be existing DHCP and DNS services.

## Architecture

[KNI UPI Lab Diagram](https://raw.githubusercontent.com/redhat-nfvpe/kni-upi-lab/master/artifacts/KNI_AF.png):

![KNI UPI Lab Diagram](https://raw.githubusercontent.com/redhat-nfvpe/kni-upi-lab/master/artifacts/KNI_AF.png)

The goal is to allow people to reproduce this environment (OCP + CNV on BM) given that you have the “same” hardware.

[Ansible playbook for deploying with IPI](https://github.com/openshift-kni/baremetal-deploy/tree/master/ansible-ipi-install)

HW is defined in an inventory file.

Adding new nodes would involve adding DHCP + DNS entries for them.

# 2020-05-06 AM

## Vision

- A working example of how you should do this (private cloud) that we can show to customers.
- Put together real use cases for real users.
- Create/destroy OCP clusters on demand.
  - Individual Clusters: The Open Data Hub team gives clusters to individual users/tenants (specific configurations, data sets, etc).
  - Shared Clusters (like the OpenStack use case)
- Kick the tires in the product.
- Multi Cluster Use Case: OpenShift clusters running on top of CNV 

## Action items

- Rick: Work with Lars to Create a “living” document for this project.
  - Start with an open repository in Git Lab. Do the documentation in markdown.
- Rick: Work with Lars to get the task tracking setup.
  - Track this effort as part of the Open Infra Labs
  - Git Lab is a  source repository that contains some project management capabilities. 
- Add BU Team Members to our weekly call:
  - Kristi Nikola
  - Naved Ansari

We need to add some missing pieces (i.e. monitoring, storage & openshift) and the architectural diagram of how we are going to put this together.

We need to identify storage requirements for the initial setup and expansion afterwards.

Initially, we could use an existing Ceph cluster, preferably an OCS environment. (what are the requirements for this?)

Hardware Resources Table (also the recommended HW and environment requirements for the CNV HTB).

| Role      | OS           | CPU (min/rec) | RAM (min/rec) | Storage (min/rec) |
|-----------|--------------|---------------|---------------|-------------------|
| Bootstrap | RHCOS/RHEL 8 | 4/4           | 16/16         | 120/120           |
| Master    | RHCOS        | 4/16          | 16/64         | 120/240           |
| Worker    | RHCOS        | 2/16          | 8/128         | 120/?             |

## Questions

- What do we lose by using local storage instead of OCS?

  Data persistent issues without shared storage.

- What do we gain by using OCS?

  Ceph cluster deployed on k8s.
