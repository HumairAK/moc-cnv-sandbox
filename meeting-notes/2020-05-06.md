# 2020-05-06 PM

## UPI vs IPI

UPI is more mature, but IPI is the preferred option, especially if you want to
add things after deploying the cluster. IPI looks like the way we’ll go.

## Requirements

Minimal required configuration:

- 1 Bootstrap VM
- 3 masters (to get the cluster to come up)
- 2 Workers (more can be added later)

The greater uniformity, the greater chance of success. Nodes within a role must be identical. NIC names (at least for provisioning NIC) must be identical across all roles.

There must be existing DHCP and DNS services.

## Architecture

[KNI UPI Lab Diagram](https://raw.githubusercontent.com/redhat-nfvpe/kni-upi-lab/master/artifacts/KNI_AF.png):

![KNI UPI Lab Diagram](https://raw.githubusercontent.com/redhat-nfvpe/kni-upi-lab/master/artifacts/KNI_AF.png)

The goal is to allow people to reproduce this environment (OCP + CNV on BM) given that you have the “same” hardware.

[Ansible playbook for deploying with IPI](https://github.com/openshift-kni/baremetal-deploy/tree/master/ansible-ipi-install)

HW is defined in an inventory file.

Adding new nodes would involve adding DHCP + DNS entries for them.

# 2020-05-06 AM

## Vision

- A working example of how you should do this (private cloud) that we can show to customers.
- Put together real use cases for real users.
- Create/destroy OCP clusters on demand.
  - Individual Clusters: The Open Data Hub team gives clusters to individual users/tenants (specific configurations, data sets, etc).
  - Shared Clusters (like the OpenStack use case)
- Kick the tires in the product.
- Multi Cluster Use Case: OpenShift clusters running on top of CNV 

## Action items

- Rick: Work with Lars to Create a “living” document for this project.
  - Start with an open repository in Git Lab. Do the documentation in markdown.
- Rick: Work with Lars to get the task tracking setup.
  - Track this effort as part of the Open Infra Labs
  - Git Lab is a  source repository that contains some project management capabilities. 
- Add BU Team Members to our weekly call:
  - Kristi Nikola
  - Naved Ansari

We need to add some missing pieces (i.e. monitoring, storage & openshift) and the architectural diagram of how we are going to put this together.

We need to identify storage requirements for the initial setup and expansion afterwards.

Initially, we could use an existing Ceph cluster, preferably an OCS environment. (what are the requirements for this?)

Hardware Resources Table (also the recommended HW and environment requirements for the CNV HTB).

| Role      | OS           | CPU (min/rec) | RAM (min/rec) | Storage (min/rec) |
|-----------|--------------|---------------|---------------|-------------------|
| Bootstrap | RHCOS/RHEL 8 | 4/4           | 16/16         | 120/120           |
| Master    | RHCOS        | 4/16          | 16/64         | 120/240           |
| Worker    | RHCOS        | 2/16          | 8/128         | 120/?             |

## Questions

- What do we lose by using local storage instead of OCS?

  Data persistent issues without shared storage.

- What do we gain by using OCS?

  Ceph cluster deployed on k8s.
