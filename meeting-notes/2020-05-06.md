# 2020/05/13

*   Yesterday we had our initial meeting (kickoff) with CNV HTB PM. We’ll meet again in two weeks after we get the Deployment Kit.
*   Repositories and task tracking
    *   Open Infrastructure Labs (High Level):[ https://gitlab.com/groups/open-infrastructure-labs/-/issues](https://gitlab.com/groups/open-infrastructure-labs/-/issues)
    *   CNV on MOC:[ https://gitlab.com/open-infrastructure-labs/moc-cnv-sandbox/-/issues](https://meet.google.com/linkredirect?authuser=0&dest=https%3A%2F%2Fgitlab.com%2Fopen-infrastructure-labs%2Fmoc-cnv-sandbox%2F-%2Fissues)
*   Designs from Israeli Team - Draft on General Dilemmas
    *   Areas covered:
        *   Storage
        *   [Metrics/monitoring/Logging](https://docs.google.com/document/d/1M5yy7sSmFht-_FR4dtfEq7K00ryNBpzd2wOgHC_lye0/edit?usp=sharing) - also  -[ https://gitlab.com/open-infrastructure-labs/nerc-architecture/-/issues/4](https://gitlab.com/open-infrastructure-labs/nerc-architecture/-/issues/4) - msd: may I add Ilana’s document to that issue?
            *   To begin, we can start with an option that is self contained and document how we could do this at a larger scale in the future.
        *   CNV / OpenShift Infrastructure
            *   Monitoring Discussion :  From: msd@bu.edu
            *   When: 9:00 AM - 10:00 AM EST May 13, 2020 
            *   Subject: Open Infra Labs/Operate First Monitoring Architecture Discussion 
            *   Location: https://bostonu.zoom.us/j/91815946494
            *   2.	Discuss https://gitlab.com/open-infrastructure-labs/nerc-architecture/-/issues/4
            *   3.	Notes and Agenda here: https://etherpad.opendev.org/p/May_13_2020Monitoring 
        *   Project Tracking Tool
            *   Currently using Jira. Our goal is to take it to gitlab.
                *   **[ ] Idan/Michael D.: **Move current tasks in Jira to GitLab.
    *   Need HW for deployment for testing and experimentation
        *   Cisco nodes are out of the question due to lack of storage drivers.
            *   Model: UCSC-C220-M3S
        *   RHEL8 support for Cisco ISCI driver?
            *   **[ ] Shon:** please explore our options here.
*   We need a single place to go for cluster monitoring, to combine data from multiple levels and do correlations.
*   Hardware wish list
    *   Starting with SDDs for our Masters (instead of HDDs)
    *   HW for our OCS-based storage solution
*   ACM team engagement
    *   [ ] Bill Burns: could you pull some people from the ACM team interested in Operate First? 
    *   [ ] Team: Use the IRC for Open Infra Labs (#openinfralabs on freenode)
*   Others
    *   
*   Next Meeting:
    *   Review answers to architecture questions posed today.
    *   A look into what we will need (ideally) to deploy the services we need.
    *   Answers to questions on CISCO nodes.

# 2020/05/12

Initial meeting for CNV HTP on MOC

*   Introductions
*   Program Intro/Overview
    *   The purpose is that we get the right feedback in terms of features, bugs, etc as we go forward with our initial GA.
    *   Expectations: A Deployment Kit to stand up the product in a standard way as quickly and frictionless as possible so that we can execute the test plan with the use cases of interest we would like you to execute as part of this program.
        *   This does not mean that we can’t go out and try other things outside of the test plan. We expect the MOC team to stretch this test plan.
        *   Our plan would be to start with an install using the prescribed plan and then figure out ways to do it automatically.
    *   A process to report issues as part of the customer portal.
    *   Our timing for having the test kit available within the next one or two weeks.
    *   We’ll have Ian engaged with getting the environment setup (fortunately w/o any hiccups due to the COVID-19 situation).
    *   [Recommended HW and environment requirements for the CNV HTB](https://docs.google.com/document/d/1ntS4Iwg6aCulVz2s8110oxp7UdeYQY7odSklEex04_Y/edit).
        *   We have 6 Dell RX620 servers.
        *   Questions on HW support:
            *   **Networking**
                *   Cisco nodes for which RHEL doesn’t have the right drivers.
                *   Do we have the correct iPXE driver support on the Dell nodes (Lars to clarify…)
                    *   NIC models
                *   [ ] Lars: Provide more specific details about our HW configuration.
            *   **Storage**
                *   Initially
                    *   Use Local Storage - 600 GB HDDs
                *   Subsequently
                    *   Use External Storage using OCS 4.4 - Ceph Clusters
            *   **Memory**
                *   128 GBs
        *   Info about network configuration
            *   We have administrative access to network infrastructure (routers, switches, etc).
        *   Info about Workloads
            *   We are looking to move our management plane to a single platform.
            *   We would love to set up the infrastructure for the New England Research Cloud to run on OpenShift Container Platform + OpenShift virtualization.
        *   Info about installation:
            *   Initially, we’ll start with the current available OCP and CNV  version. This environment is expected to be burned down, however it would be better to do the upgrade.
            *   Subsequently, we’ll move up to OCP 4.5 / CNV 2.4.
    *   Next Steps
        *   Get started with the OCP IPI BM installation as soon as possible.
        *   Expect the Deployment Kit to be available within two weeks.

**[ ] Rick: **set another follow up meeting in two weeks.



# 2020-05-06 PM

## UPI vs IPI

UPI is more mature, but IPI is the preferred option, especially if you want to
add things after deploying the cluster. IPI looks like the way we’ll go.

## Requirements

Minimal required configuration:

- 1 Bootstrap VM
- 3 masters (to get the cluster to come up)
- 2 Workers (more can be added later)

The greater uniformity, the greater chance of success. Nodes within a role must be identical. NIC names (at least for provisioning NIC) must be identical across all roles.

There must be existing DHCP and DNS services.

## Architecture

[KNI UPI Lab Diagram](https://raw.githubusercontent.com/redhat-nfvpe/kni-upi-lab/master/artifacts/KNI_AF.png):

![KNI UPI Lab Diagram](https://raw.githubusercontent.com/redhat-nfvpe/kni-upi-lab/master/artifacts/KNI_AF.png)

The goal is to allow people to reproduce this environment (OCP + CNV on BM) given that you have the “same” hardware.

[Ansible playbook for deploying with IPI](https://github.com/openshift-kni/baremetal-deploy/tree/master/ansible-ipi-install)

HW is defined in an inventory file.

Adding new nodes would involve adding DHCP + DNS entries for them.

# 2020-05-06 AM

## Vision

- A working example of how you should do this (private cloud) that we can show to customers.
- Put together real use cases for real users.
- Create/destroy OCP clusters on demand.
  - Individual Clusters: The Open Data Hub team gives clusters to individual users/tenants (specific configurations, data sets, etc).
  - Shared Clusters (like the OpenStack use case)
- Kick the tires in the product.
- Multi Cluster Use Case: OpenShift clusters running on top of CNV 

## Action items

- Rick: Work with Lars to Create a “living” document for this project.
  - Start with an open repository in Git Lab. Do the documentation in markdown.
- Rick: Work with Lars to get the task tracking setup.
  - Track this effort as part of the Open Infra Labs
  - Git Lab is a  source repository that contains some project management capabilities. 
- Add BU Team Members to our weekly call:
  - Kristi Nikola
  - Naved Ansari

We need to add some missing pieces (i.e. monitoring, storage & openshift) and the architectural diagram of how we are going to put this together.

We need to identify storage requirements for the initial setup and expansion afterwards.

Initially, we could use an existing Ceph cluster, preferably an OCS environment. (what are the requirements for this?)

Hardware Resources Table (also the recommended HW and environment requirements for the CNV HTB).

| Role      | OS           | CPU (min/rec) | RAM (min/rec) | Storage (min/rec) |
|-----------|--------------|---------------|---------------|-------------------|
| Bootstrap | RHCOS/RHEL 8 | 4/4           | 16/16         | 120/120           |
| Master    | RHCOS        | 4/16          | 16/64         | 120/240           |
| Worker    | RHCOS        | 2/16          | 8/128         | 120/?             |

## Questions

- What do we lose by using local storage instead of OCS?

  Data persistent issues without shared storage.

- What do we gain by using OCS?

  Ceph cluster deployed on k8s.
